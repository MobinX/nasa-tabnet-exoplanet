{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13236864,"sourceType":"datasetVersion","datasetId":8388075,"isSourceIdPinned":false},{"sourceId":13238789,"sourceType":"datasetVersion","datasetId":8389131,"isSourceIdPinned":false},{"sourceId":596582,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":446745,"modelId":463209}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\nimport kagglehub\nmobinchowdhury_k2_nasa_path = kagglehub.dataset_download('mobinchowdhury/k2-nasa')\nmobinchowdhury_exoplanet_dataset_path = kagglehub.dataset_download('mobinchowdhury/exoplanet-dataset')\n\nprint('Data source import complete.')\n","metadata":{"id":"z40qFdHHl95N","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d9a7dc4-b15f-41a8-e0fe-e6e9e576e3ad","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:25:02.041511Z","iopub.execute_input":"2025-10-03T08:25:02.041794Z","iopub.status.idle":"2025-10-03T08:25:02.405578Z","shell.execute_reply.started":"2025-10-03T08:25:02.041773Z","shell.execute_reply":"2025-10-03T08:25:02.404853Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install pandas scikit-learn torch pytorch-tabnet joblib\n!pip install onnx onnxruntime","metadata":{"trusted":true,"id":"gvydL8fxl95V","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fcc4409b-b526-4643-f808-c2a4d7278db6","execution":{"iopub.status.busy":"2025-10-03T08:24:48.479361Z","iopub.execute_input":"2025-10-03T08:24:48.480104Z","iopub.status.idle":"2025-10-03T08:24:51.689637Z","shell.execute_reply.started":"2025-10-03T08:24:48.480076Z","shell.execute_reply":"2025-10-03T08:24:51.688883Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install --upgrade scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 1: Import Modules and Prep Data","metadata":{"id":"GtFfzLkDRdoK"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom imblearn.over_sampling import RandomOverSampler\nimport torch\n\n# ---------------------------\n# Step 1: Load and merge datasets\n# ---------------------------\ndef load_nasa_dataset(tess_file, kepler_file, k2_file):\n    # Load CSVs, skip commented lines\n    tess_df = pd.read_csv(tess_file, comment='#')\n    kepler_df = pd.read_csv(kepler_file, comment='#')\n    k2_df = pd.read_csv(k2_file, comment='#')\n\n    # Define the \"canonical\" columns we want in the merged dataset\n    common_cols = ['planet_name', 'disposition', 'orbital_period', 'planet_radius',\n                   'equilibrium_temp', 'insolation_flux', 'transit_depth', 'transit_duration',\n                   'stellar_teff', 'stellar_logg', 'stellar_radius', 'ra', 'dec']\n\n    # Helper to map CSV-specific column names to canonical names\n    def rename_columns(df, mapping):\n        df = df.copy()\n        for k, v in mapping.items():\n            if k in df.columns:\n                df.rename(columns={k: v}, inplace=True)\n        return df\n    def normalize_disposition(df, source='generic'):\n        df = df.copy()\n        if 'disposition' in df.columns:\n            if source == 'tess':  # TESS uses CP/PC/FP\n                mapping = {'CP': 'CONFIRMED', 'PC': 'CANDIDATE', 'FP': 'FALSE POSITIVE'}\n                df['disposition'] = df['disposition'].map(mapping)\n            else:  # Kepler/K2 already use text like CONFIRMED/CANDIDATE/FALSE POSITIVE\n                df['disposition'] = df['disposition'].replace({\n                    'CANDIDATE': 'CANDIDATE',\n                    'CONFIRMED': 'CONFIRMED',\n                    'FALSE POSITIVE': 'FALSE POSITIVE',\n                    'REFUTED': 'FALSE POSITIVE'\n                })\n        return df\n\n    # Kepler mapping\n    kepler_map = {\n        'kepler_name': 'planet_name',\n        'koi_disposition': 'disposition',\n        'koi_period': 'orbital_period',\n        'koi_prad': 'planet_radius',\n        'koi_teq': 'equilibrium_temp',\n        'koi_insol': 'insolation_flux',\n        'koi_depth': 'transit_depth',\n        'koi_duration': 'transit_duration',\n        'koi_steff': 'stellar_teff',\n        'koi_slogg': 'stellar_logg',\n        'koi_srad': 'stellar_radius'\n    }\n\n    # K2 mapping\n    k2_map = {\n        'pl_name': 'planet_name',\n        'disposition': 'disposition',\n        'pl_orbper': 'orbital_period',\n        'pl_rade': 'planet_radius',\n        'pl_eqt': 'equilibrium_temp',\n        'pl_insol': 'insolation_flux',\n        'pl_rade': 'planet_radius',\n        'st_teff': 'stellar_teff',\n        'st_logg': 'stellar_logg',\n        'st_rad': 'stellar_radius'\n    }\n\n    # TESS mapping\n    tess_map = {\n        'toi': 'planet_name',\n        'tfopwg_disp': 'disposition',\n        'pl_orbper': 'orbital_period',\n        'pl_rade': 'planet_radius',\n        'pl_eqt': 'equilibrium_temp',\n        'pl_insol': 'insolation_flux',\n        'pl_trandep': 'transit_depth',\n        'pl_trandurh': 'transit_duration',\n        'st_teff': 'stellar_teff',\n        'st_logg': 'stellar_logg',\n        'st_rad': 'stellar_radius'\n    }\n\n    # Rename columns\n    kepler_df = rename_columns(kepler_df, kepler_map)\n    k2_df = rename_columns(k2_df, k2_map)\n    tess_df = rename_columns(tess_df, tess_map)\n    kepler_df = normalize_disposition(kepler_df, 'kepler')\n    k2_df = normalize_disposition(k2_df, 'k2')\n    tess_df = normalize_disposition(tess_df, 'tess')\n\n    # Keep only columns that exist in each dataframe\n    kepler_df = kepler_df[[c for c in common_cols if c in kepler_df.columns]]\n    k2_df = k2_df[[c for c in common_cols if c in k2_df.columns]]\n    tess_df = tess_df[[c for c in common_cols if c in tess_df.columns]]\n\n    # Merge all datasets\n    merged_df = pd.concat([kepler_df, k2_df, tess_df], ignore_index=True)\n\n    return merged_df\n\n# Example usage\nmerged_df = load_nasa_dataset(\n    mobinchowdhury_exoplanet_dataset_path+\"/tess.csv\",\n    mobinchowdhury_exoplanet_dataset_path+\"/keplar.csv\",\n    mobinchowdhury_k2_nasa_path+\"/k2panda.csv\"\n)\n\nprint(\"Merged dataset shape:\", merged_df.shape)\nprint(merged_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:25:08.261878Z","iopub.execute_input":"2025-10-03T08:25:08.262428Z","iopub.status.idle":"2025-10-03T08:25:08.635879Z","shell.execute_reply.started":"2025-10-03T08:25:08.262405Z","shell.execute_reply":"2025-10-03T08:25:08.635174Z"},"id":"KvKmZWAnl95W","outputId":"37fbb30e-34cc-4561-8ea1-08be7d517ccf","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"Merged dataset shape: (21267, 13)\n    planet_name     disposition  orbital_period  planet_radius  \\\n0  Kepler-227 b       CONFIRMED        9.488036           2.26   \n1  Kepler-227 c       CONFIRMED       54.418383           2.83   \n2           NaN       CANDIDATE       19.899140          14.60   \n3           NaN  FALSE POSITIVE        1.736952          33.46   \n4  Kepler-664 b       CONFIRMED        2.525592           2.75   \n\n   equilibrium_temp  insolation_flux  transit_depth  transit_duration  \\\n0             793.0            93.59          615.8           2.95750   \n1             443.0             9.11          874.8           4.50700   \n2             638.0            39.30        10829.0           1.78220   \n3            1395.0           891.96         8079.2           2.40641   \n4            1406.0           926.16          603.3           1.65450   \n\n   stellar_teff  stellar_logg  stellar_radius         ra        dec  \n0        5455.0         4.467           0.927  291.93423  48.141651  \n1        5455.0         4.467           0.927  291.93423  48.141651  \n2        5853.0         4.544           0.868  297.00482  48.134129  \n3        5805.0         4.564           0.791  285.53461  48.285210  \n4        6031.0         4.438           1.046  288.75488  48.226200  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Step 2: Polish data and split for test and train","metadata":{"id":"7G8QtkbhRrDM"}},{"cell_type":"code","source":"merged_df = merged_df.dropna(subset=['disposition'])\n\n# Features and target\n\ncandidate_df = merged_df[merged_df['disposition'] == 'CANDIDATE'].copy()\n\n# Drop unused columns\ncandidate_X = candidate_df.drop(columns=['disposition', 'planet_name'])\n\n\n\nfiltered_df = merged_df[merged_df['disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()\nX = filtered_df.drop(columns=['disposition', 'planet_name'])\n# Fill missing values in numeric columns with median\nX = X.fillna(X.median())\n\n# Standardize numeric features\n# scaler = StandardScaler()\n# X = scaler.fit_transform(X)\n\n# ---------------------------\n# Step 3: Handle class imbalance\n# ---------------------------\n# ros = RandomOverSampler(random_state=42)\n# X_res, y_res = ros.fit_resample(X, y)\n# Encode target labels\n\n\ny = filtered_df['disposition']\nle = LabelEncoder()\ny = le.fit_transform(y)  # e.g., 'CONFIRMED' -> 0, 'CANDIDATE' -> 1, etc.\nfeature_names = list(X.columns)\n# Apply same preprocessing \ncandidate_X = candidate_X.fillna(X.median())\n\n# Make sure columns are in the same order as training\ncandidate_X = candidate_X[feature_names]\n\n# Convert to numpy\ncandidate_input = candidate_X.values\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X.values, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train distribution:\", np.bincount(y_train))\nprint(\"y_test distribution:\", np.bincount(y_test))\nprint(y)\ncandidate_df\nle.classes_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:27:37.573948Z","iopub.execute_input":"2025-10-03T08:27:37.574666Z","iopub.status.idle":"2025-10-03T08:27:37.619122Z","shell.execute_reply.started":"2025-10-03T08:27:37.574637Z","shell.execute_reply":"2025-10-03T08:27:37.618535Z"},"id":"6tlUD7o-l95Z","outputId":"c154401c-0581-4b8f-a1d6-d979f6b9504d","colab":{"base_uri":"https://localhost:8080/","height":535}},"outputs":[{"name":"stdout","text":"X_train shape: (9675, 11)\nX_test shape: (2419, 11)\ny_train distribution: [4595 5080]\ny_test distribution: [1149 1270]\n[0 0 1 ... 1 1 1]\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array(['CONFIRMED', 'FALSE POSITIVE'], dtype=object)"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Step 3: Train the model & Save the pkl\n","metadata":{"id":"24iG4ODATSL1"}},{"cell_type":"code","source":"# ============================\n# 3. Train TabNet\n# ============================\n# ---------------------------\n# Step 5: TabNet training\n# ---------------------------\nscheduler_params = {\"step_size\": 50, \"gamma\": 0.9}\nclf = TabNetClassifier(\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=1e-3),\n    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n    scheduler_params=scheduler_params,\n    n_steps=8, n_d=16, n_a=16, gamma=1.5,\n    mask_type=\"entmax\"\n)\n\nclf.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    max_epochs=200,\n    patience=200,\n    batch_size=256,\n    virtual_batch_size=128,\n    num_workers=0,\n    drop_last=False\n)\n\nimport joblib\n# Save the model with map_location='cpu' to ensure it can be loaded on a CPU-only machine\njoblib.dump(clf, \"tabnet_exoplanet.pkl\") # map_location='cpu' is not a joblib argument\njoblib.dump(y, \"target_encoder.pkl\")\n\nprint(\"✅ Model and encoder saved!\")","metadata":{"trusted":true,"id":"Gal4xYE1l95d","outputId":"53cba90b-de15-4868-80f7-83a398e1e331","colab":{"base_uri":"https://localhost:8080/","height":467}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n```\nEvaluate the model\n```\n\n\n\n\n","metadata":{"id":"5gBifaG1Tbxr"}},{"cell_type":"code","source":"# ============================\n# 4. Evaluation\n# ============================\n# Step 6: Evaluation\n# ---------------------------\ny_pred = clf.predict(X_test)\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true,"id":"OyVawHj0l95e"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\nimport torch\n\n# Load model\n# Specify map_location='cpu' if the model was saved from a GPU machine\n# and you are loading it on a CPU-only machine.\nclf = joblib.load(\"/kaggle/input/tabnet_exoplannet/pytorch/default/1/tabnet_exoplanet.pkl\")","metadata":{"id":"imzcRcJnouV_","colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"ee398bcf-3183-47b0-9cda-8c9ecf73f1fb","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:27:44.911637Z","iopub.execute_input":"2025-10-03T08:27:44.912458Z","iopub.status.idle":"2025-10-03T08:27:45.407623Z","shell.execute_reply.started":"2025-10-03T08:27:44.912432Z","shell.execute_reply":"2025-10-03T08:27:45.407034Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"**Evaluate model agaisnt candidate plannet data to figure out which candate has high possibility of being confirmed **","metadata":{}},{"cell_type":"code","source":"candidate_probs = clf.predict_proba(candidate_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:27:48.030233Z","iopub.execute_input":"2025-10-03T08:27:48.030529Z","iopub.status.idle":"2025-10-03T08:27:49.296031Z","shell.execute_reply.started":"2025-10-03T08:27:48.030511Z","shell.execute_reply":"2025-10-03T08:27:49.295390Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import json\n\n# Add probabilities to candidate_df safely\nfor i, class_name in enumerate(le.classes_):\n    col_name = f\"prob_{class_name.replace(' ', '_').lower()}\"\n    candidate_df[col_name] = candidate_probs[:, i]\n\n# Print first 5 rows\nprint(candidate_df.head())\n\n# Export the entire DataFrame to JSON (records = list of dicts)\ncandidate_json = candidate_df.to_json(orient='records')\n\n# Optional: save to file\nwith open(\"/kaggle/working/candidate_predictions.json\", \"w\") as f:\n    f.write(candidate_json)\n\nprint(\"✅ Exported candidate predictions to JSON\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:29:22.982322Z","iopub.execute_input":"2025-10-03T08:29:22.982665Z","iopub.status.idle":"2025-10-03T08:29:23.020161Z","shell.execute_reply.started":"2025-10-03T08:29:22.982644Z","shell.execute_reply":"2025-10-03T08:29:23.019415Z"}},"outputs":[{"name":"stdout","text":"   planet_name disposition  orbital_period  planet_radius  equilibrium_temp  \\\n2          NaN   CANDIDATE       19.899140          14.60             638.0   \n58         NaN   CANDIDATE       40.419504           7.51             467.0   \n62         NaN   CANDIDATE        7.240661          19.45             734.0   \n63         NaN   CANDIDATE        3.435916           0.55            1272.0   \n84         NaN   CANDIDATE       10.181584           7.73             812.0   \n\n    insolation_flux  transit_depth  transit_duration  stellar_teff  \\\n2             39.30        10829.0            1.7822        5853.0   \n58            11.29         6256.0            3.3620        5446.0   \n62            68.63          556.4            0.5580        5005.0   \n63           617.61           23.2            3.1330        5779.0   \n84           102.91         5741.1            3.5089        5988.0   \n\n    stellar_logg  stellar_radius         ra        dec  prob_confirmed  \\\n2          4.544           0.868  297.00482  48.134129        0.347529   \n58         4.507           0.781  294.31686  50.080231        0.663197   \n62         4.595           0.765  293.83331  50.230350        0.597596   \n63         4.339           1.087  287.88733  46.276241        0.523592   \n84         4.541           0.836  291.28195  38.241669        0.497521   \n\n    prob_false_positive  \n2              0.652471  \n58             0.336803  \n62             0.402404  \n63             0.476408  \n84             0.502479  \n✅ Exported candidate predictions to JSON\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nimport torch\n\n# Feature names used by your model (excluding planet_name and disposition)\nfeature_names = ['orbital_period', 'planet_radius', 'equilibrium_temp', 'insolation_flux', \n                 'transit_depth', 'transit_duration', 'stellar_teff', 'stellar_logg', \n                 'stellar_radius', 'ra', 'dec']\n\n# Single candidate's data\ncandidate_data = [6.444, 2.460, 712.281, 60.831, 1121.951, 1.931, 4803.0, 4.521, 0.737, 296.004, -47.562]\n\n# Convert to numpy array and reshape to (1, n_features)\ncandidate_array = np.array(candidate_data, dtype=np.float32).reshape(1, -1)\n\n# Convert to torch tensor (if your model is on CPU)\ncandidate_tensor = torch.tensor(candidate_array)\n\n\n# Predict probabilities using the trained TabNetClassifier\ncandidate_probs = clf.predict_proba(candidate_tensor.numpy())  # clf expects numpy array\n\n# Assuming you already have your LabelEncoder `le` to map classes\nfor i, class_name in enumerate(le.classes_):\n    print(f\"Probability of {class_name}: {candidate_probs[0, i]:.4f}\")\n\n# Specifically for CONFIRMED\nconfirmed_index = np.where(le.classes_ == \"CONFIRMED\")[0][0]\nprint(f\"\\nProbability of CONFIRMED: {candidate_probs[0, confirmed_index]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:45:08.214724Z","iopub.execute_input":"2025-10-03T08:45:08.215072Z","iopub.status.idle":"2025-10-03T08:45:08.250569Z","shell.execute_reply.started":"2025-10-03T08:45:08.215047Z","shell.execute_reply":"2025-10-03T08:45:08.249989Z"}},"outputs":[{"name":"stdout","text":"Probability of CONFIRMED: 0.9181\nProbability of FALSE POSITIVE: 0.0819\n\nProbability of CONFIRMED: 0.9181\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import json\n\n# Convert the entire candidate_df to list of dictionaries\ncandidate_list = candidate_df.to_dict(orient=\"records\")\n\n# Export to JSON\nwith open(\"/kaggle/working/top-candidate_data.json\", \"w\") as f:\n    json.dump(candidate_list, f, indent=4)\n\n# Optional: print first 5 entries nicely\nprint(json.dumps(candidate_list[:5], indent=4))\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Candidate cols:\", candidate_df.columns.tolist())\nprint(\"Train cols:\", X.columns.tolist())\nprint(\"Shape candidate:\", candidate_df.shape, \"train:\", X_train.shape[1])\nprint(candidate_df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\n\n\n# Get TabNet feature importances\nimportances = clf.feature_importances_\n\n# Combine into DataFrame\nfeat_imp = pd.DataFrame({\n    \"feature\": feature_names,\n    \"importance\": importances\n}).sort_values(by=\"importance\", ascending=False)\n\nprint(feat_imp)\n\n# Plot top 15 features\nplt.figure(figsize=(10, 6))\nplt.barh(feat_imp[\"feature\"].head(15), feat_imp[\"importance\"].head(15))\nplt.gca().invert_yaxis()  # so most important is on top\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"TabNet Feature Importances\")\nplt.show()\n","metadata":{"id":"eftw0od1qGSY","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport pandas as pd\n\n# Suppose X is your DataFrame of features\nfeature_names = X.columns.tolist()\nimportances = clf.feature_importances_  # TabNet feature importances\n\n# Combine names and importances into a dictionary\nfeat_dict = {name: float(imp) for name, imp in zip(feature_names, importances)}\n\n# Export to JSON\nwith open(\"tabnet_feature_importances.json\", \"w\") as f:\n    json.dump(feat_dict, f, indent=4)\n\nprint(\"Feature importances saved to tabnet_feature_importances.json\")\n","metadata":{"id":"lzBvZ1cUH95b","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport joblib\n\n# Load your TabNet model\nclf = joblib.load(\"tabnet_exoplanet.pkl\")\n\n# Take a small batch from your data (replace X_test with your test data)\nsample = torch.tensor(X_test[:5], dtype=torch.float32)\n\n# Get the model output\noutput_proba = clf.predict_proba(sample)  # probabilities\noutput_label = clf.predict(sample)        # class labels\n\nprint(\"Output shape (predict_proba):\", output_proba.shape)\nprint(\"Sample output (predict_proba):\\n\", output_proba)\n\nprint(\"Output shape (predict):\", output_label.shape)\nprint(\"Sample output (predict):\\n\", output_label)\n\n# Determine format\nif output_proba.shape[1] == 1:\n    print(\"Model outputs single probability per sample (sigmoid).\")\nelif output_proba.shape[1] == 2:\n    print(\"Model outputs two probabilities (softmax over two classes).\")\nelse:\n    print(\"Unknown output format!\")\n","metadata":{"id":"Bybzh7k78VbR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Get explanations (feature masks) for X_test\nexplain_matrix, masks = clf.explain(X_test)\n\n# Example: check the first test sample\nsample_idx = 7\nsample_explain = explain_matrix[sample_idx]\n\n# Plot local feature importance for that sample\nplt.figure(figsize=(10,6))\nplt.barh(range(len(feature_names)), sample_explain, align=\"center\")\nplt.yticks(range(len(feature_names)), feature_names)\nplt.xlabel(\"Importance for this sample\")\nplt.title(f\"TabNet Local Explanation for Sample {sample_idx}\")\nplt.show()\n","metadata":{"id":"xzkx-uNptpgJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Average importance across all samples\nglobal_importance = np.mean(explain_matrix, axis=0)\n\npd.DataFrame({\n    \"feature\": feature_names,\n    \"mean_importance\": global_importance\n}).sort_values(\"mean_importance\", ascending=False)\n","metadata":{"id":"SUzC5tYhuIpt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Compute min and max for each feature\nfeature_ranges = {}\nfor col in X.columns:\n    min_val = X[col].min()\n    max_val = X[col].max()\n    feature_ranges[col] = {\"range\": [float(min_val), float(max_val)]}\n\n# Save as JSON\nwith open(\"feature_ranges.json\", \"w\") as f:\n    json.dump(feature_ranges, f, indent=4)\n\nprint(feature_ranges)","metadata":{"id":"WMslsvmk-K8y","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Make sure the model is in eval mode\n\n# Suppose X_train is your training data (numpy or pandas)\n# If X_train is a DataFrame:\n\n# If X_train is a numpy array and you have a separate feature_names list, use it\n# feature_names = [\"feat1\", \"feat2\", ..., \"featN\"]\n\n# Create a dummy input with the same number of features\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Move the model to device\nclf.network.to(device)\nclf.network.eval()  # important for export\n\n# Create dummy input on the same device\ndummy_input = torch.tensor(np.zeros((1, X.shape[1])), dtype=torch.float32).to(device)\n\n# Export to ONNX\ntorch.onnx.export(\n    clf.network, # Export the underlying PyTorch network\n    dummy_input,\n    \"/kaggle/working/tabnet_exoplanet.onnx\",\n    export_params=True,  # store the trained parameters inside the model file\n    opset_version=11,    # the ONNX version to export the model to\n    do_constant_folding=True, # whether to execute constant folding for optimization\n    input_names=['input'],   # the model's input names\n    output_names=['output'], # the model's output names\n    dynamic_axes={'input': {0: 'batch_size'},    # variable length axes\n                  'output': {0: 'batch_size'}})\n\nprint(\"Export completed. ONNX file ready for web use.\")\n","metadata":{"id":"m96F6VUzC_s1","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:31:22.401178Z","iopub.execute_input":"2025-10-03T08:31:22.402135Z","iopub.status.idle":"2025-10-03T08:31:25.060664Z","shell.execute_reply.started":"2025-10-03T08:31:22.402107Z","shell.execute_reply":"2025-10-03T08:31:25.060016Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/tab_network.py:35: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n  chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n","output_type":"stream"},{"name":"stdout","text":"Export completed. ONNX file ready for web use.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import onnxruntime as ort\n\n# Load the model\nsession = ort.InferenceSession(\"/kaggle/working/tabnet_exoplanet.onnx\")\ninput_name = session.get_inputs()[0].name\ninput_name\noutputs = session.run(None, {input_name: candidate_tensor.numpy()})\noutputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:51:12.453055Z","iopub.execute_input":"2025-10-03T08:51:12.453398Z","iopub.status.idle":"2025-10-03T08:51:12.666874Z","shell.execute_reply.started":"2025-10-03T08:51:12.453379Z","shell.execute_reply":"2025-10-03T08:51:12.666233Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[array([[2.9333153, 0.5169577]], dtype=float32),\n array(-0.5776607, dtype=float32)]"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(outputs[0])\nsession.get_outputs()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T08:52:17.139320Z","iopub.execute_input":"2025-10-03T08:52:17.139971Z","iopub.status.idle":"2025-10-03T08:52:17.145720Z","shell.execute_reply.started":"2025-10-03T08:52:17.139948Z","shell.execute_reply":"2025-10-03T08:52:17.145027Z"}},"outputs":[{"name":"stdout","text":"[[2.9333153 0.5169577]]\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg at 0x7da264df8b70>,\n <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg at 0x7da264e10db0>]"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"import numpy as np\n\n# The raw output (logits) from your ONNX session\nonnx_output_logits = outputs[0]\n\n# Apply softmax function\nexp_logits = np.exp(onnx_output_logits)\nsoftmax_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n\nprint(\"Raw ONNX output (logits):\\n\", onnx_output_logits)\nprint(\"Softmax probabilities:\\n\", softmax_probs)\n\n# Assuming le (LabelEncoder) is still available\nif 'le' in globals():\n    print(\"\\nProbabilities with class names:\")\n    for i, class_name in enumerate(le.classes_):\n        print(f\"{class_name}: {softmax_probs[0, i]:.4f}\")\nelse:\n    print(\"\\nLabelEncoder 'le' not found. Cannot display class names.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T09:00:57.459140Z","iopub.execute_input":"2025-10-03T09:00:57.459994Z","iopub.status.idle":"2025-10-03T09:00:57.466186Z","shell.execute_reply.started":"2025-10-03T09:00:57.459965Z","shell.execute_reply":"2025-10-03T09:00:57.465310Z"}},"outputs":[{"name":"stdout","text":"Raw ONNX output (logits):\n [[2.9333153 0.5169577]]\nSoftmax probabilities:\n [[0.9180662  0.08193382]]\n\nProbabilities with class names:\nCONFIRMED: 0.9181\nFALSE POSITIVE: 0.0819\n","output_type":"stream"}],"execution_count":25}]}